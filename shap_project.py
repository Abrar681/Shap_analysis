# -*- coding: utf-8 -*-
"""shap_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uK9G59P2OCQzRuEmEHTUI4ckliaxorRZ
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pycox torchtuples --quiet
!pip install torch --quiet
!pip install numpy pandas scikit-learn matplotlib seaborn lifelines pycox torchtuples --quiet
!pip install torch --quiet
!pip install lifelines
!pip install lifelines scikit-learn matplotlib joblib
# Optional (for ElasticNet Cox)
!pip install scikit-survival

# Step 2: Load dataset
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import torch
import torchtuples as tt
import torch.nn as nn
from pycox.models import CoxPH
from pycox.models import CoxPH
import shap
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from pycox.models import CoxPH
from sklearn.model_selection import train_test_split
from pycox.evaluation import EvalSurv
import scipy.integrate
import os
from lifelines import CoxPHFitter
from lifelines import KaplanMeierFitter
import matplotlib.pyplot as plt
from lifelines.utils import concordance_index
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt

data_path = '/content/drive/MyDrive/project/data/survival_dataset.csv'
df=pd.read_csv(data_path)
df.head()

# Shape of dataset
print("Shape:", df.shape)
print("--------------------------")
# Info about columns and datatypes
df.info()
print("-----------------------------")
# Check for missing values
df.isnull().sum()

# Event count
event_counts = df['event'].value_counts()
event_counts
# Censoring rate
censoring_rate = (df['event'] == 0).mean()
print("Censoring Rate:", censoring_rate)

df.describe()

num_cols = ["age", "bmi", "bp", "comorbidity"]

plt.figure(figsize=(12,8))
for i, col in enumerate(num_cols, 1):
    plt.subplot(2, 2, i)
    sns.histplot(df[col], kde=True)
    plt.title(f"Distribution of {col}")
plt.tight_layout()
plt.show()

from lifelines import KaplanMeierFitter
import matplotlib.pyplot as plt

kmf = KaplanMeierFitter()

plt.figure(figsize=(8,6))
kmf.fit(durations=df['time'], event_observed=df['event'])
kmf.plot()
plt.title("Overall Survival Curve")
plt.xlabel("Time")
plt.ylabel("Survival Probability")
plt.show()

plt.figure(figsize=(8,6))

for group in df['treatment'].unique():
    kmf.fit(
        durations=df[df['treatment'] == group]['time'],
        event_observed=df[df['treatment'] == group]['event'],
        label=f"Treatment {group}"
    )
    kmf.plot()

plt.title("Survival Curve by Treatment")
plt.xlabel("Time")
plt.ylabel("Survival Probability")
plt.show()

plt.figure(figsize=(8,6))

for group in df['sex'].unique():
    kmf.fit(
        durations=df[df['sex'] == group]['time'],
        event_observed=df[df['sex'] == group]['event'],
        label=f"Sex {group}"
    )
    kmf.plot()

plt.title("Survival Curve by Sex (0=Female, 1=Male)")
plt.xlabel("Time")
plt.ylabel("Survival Probability")
plt.show()

plt.figure(figsize=(8,6))

for com in sorted(df['comorbidity'].unique()):
    kmf.fit(
        durations=df[df['comorbidity'] == com]['time'],
        event_observed=df[df['comorbidity'] == com]['event'],
        label=f"Comorbidity {com}"
    )
    kmf.plot()

plt.title("Survival by Comorbidity Level")
plt.xlabel("Time")
plt.ylabel("Survival Probability")
plt.show()

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

train_df.shape, test_df.shape

cph = CoxPHFitter()

cph.fit(
    train_df,
    duration_col="time",
    event_col="event"
)

cph.print_summary()

# Predict partial hazards for test data
test_pred = cph.predict_partial_hazard(test_df)

# C-index
c_index = concordance_index(
    test_df["time"],
    -test_pred,
    test_df["event"]
)

print("Test C-index:", c_index)

cph.check_assumptions(train_df, p_value_threshold=0.05)

def cv_penalizer_lifelines(df_train, penalizer_grid, n_splits=4, random_state=0):
    """
    Returns dict penalizer -> mean C-index (CV) using lifelines CoxPHFitter.
    """
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    results = {}
    Xy = df_train.reset_index(drop=True)
    for pen in penalizer_grid:
        cidxs = []
        for train_idx, val_idx in kf.split(Xy):
            train = Xy.iloc[train_idx]
            val = Xy.iloc[val_idx]
            cph = CoxPHFitter(penalizer=pen)
            # fit
            cph.fit(train, duration_col="time", event_col="event", show_progress=False)
            preds = cph.predict_partial_hazard(val.drop(columns=["time", "event"]))
            cidx = concordance_index(val["time"], -preds, val["event"])
            cidxs.append(cidx)
        results[pen] = np.mean(cidxs)
        print(f"penalizer={pen:.5f} -> CV C-index={results[pen]:.4f}")
    return results

# Define grid and run CV
pen_grid = [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]
cv_results = cv_penalizer_lifelines(train_df, pen_grid, n_splits=4)
best_pen = max(cv_results, key=cv_results.get)
print("Best penalizer:", best_pen, "CV C-index:", cv_results[best_pen])

import joblib
os.makedirs("../results/model_checkpoints", exist_ok=True)

best_cph = CoxPHFitter(penalizer=best_pen)
best_cph.fit(train_df, duration_col="time", event_col="event")
best_cph.print_summary()

# Save model (joblib recommended for lifelines objects)
joblib.dump(best_cph, "../results/model_checkpoints/cox_lifelines_penalized.pkl")
print("Saved model to ../results/model_checkpoints/cox_lifelines_penalized.pkl")

X_test = test_df.drop(columns=["time", "event"])
test_partial_hazard = best_cph.predict_partial_hazard(X_test)
test_cindex = concordance_index(test_df["time"], -test_partial_hazard, test_df["event"])
print("Test C-index (penalized Cox):", test_cindex)

# Attach predictions for inspection
test_df = test_df.copy()
test_df["partial_hazard"] = np.ravel(test_partial_hazard)
test_df.sort_values("partial_hazard", ascending=False).head(8)

# Fit an unpenalized Cox for comparison (optional)
cph_unpen = CoxPHFitter(penalizer=0.0)
cph_unpen.fit(train_df, duration_col="time", event_col="event")
coef_df = pd.DataFrame({
    "feature": best_cph.params_.index,
    "coef_penalized": best_cph.params_.values,
    "coef_unpenalized": cph_unpen.params_.loc[best_cph.params_.index].values
}).reset_index(drop=True)

# show table
display(coef_df.sort_values("coef_penalized", key=abs, ascending=False).head(20))

# Plot coefficients
plt.figure(figsize=(8,5))
coef_df.set_index("feature")[["coef_unpenalized","coef_penalized"]].plot(kind="bar")
plt.title("Cox Coefficients: unpenalized vs penalized")
plt.ylabel("Coefficient")
plt.tight_layout()
plt.show()

# 1 Make a clean copy of the dataframe (remove partial_hazard if it exists)
clean_df = df.drop(columns=['partial_hazard'], errors='ignore')

# 2 Instantiate ElasticNet Cox model
cph_en = CoxPHFitter(
    penalizer=0.1,    # controls overall regularization strength
    l1_ratio=0.5      # 0 = Ridge, 1 = LASSO, 0.5 = ElasticNet
)

# 3 Fit the model
cph_en.fit(clean_df, duration_col='time', event_col='event')

# 4 See results
cph_en.print_summary()

df = pd.read_csv("/content/drive/MyDrive/project/data/survival_dataset.csv")
df = df.drop(columns=['partial_hazard'], errors='ignore')

# Features and targets
feature_names = [c for c in df.columns if c not in ['time','event']]
X = df[feature_names].values.astype('float32')
times = df['time'].values.astype('float32')
events = df['event'].values.astype('int32')

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_imp = imputer.fit_transform(X)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp).astype('float32')

# Train/val/test split
X_train, X_temp, t_train, t_temp, e_train, e_temp = train_test_split(
    X_scaled, times, events, test_size=0.3, random_state=42, stratify=events
)
X_val, X_test, t_val, t_test, e_val, e_test = train_test_split(
    X_temp, t_temp, e_temp, test_size=0.5, random_state=42, stratify=e_temp
)

print("Train:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)

# Network architecture
in_features = X_train.shape[1]
num_nodes = [64, 32]
out_features = 1
net = tt.practical.MLPVanilla(in_features, num_nodes, out_features,batch_norm=True, dropout=0.1)

# CoxPH model
model = CoxPH(net, torch.optim.Adam)
model.optimizer.set_lr(1e-3)

# Fit model — IMPORTANT: pass (durations, events) as a tuple
log = model.fit(
    X_train,
    (t_train, e_train),   # <-- fixed here
    batch_size=32,
    epochs=100,
    val_data=(X_val, (t_val, e_val)),  # validation also as tuple
    verbose=True
)

# Compute baseline hazards
model.compute_baseline_hazards()

# Patch for SciPy >=1.11
if not hasattr(scipy.integrate, "simps"):
    scipy.integrate.simps = scipy.integrate.simpson

# Predict survival curves for test set
surv = model.predict_surv_df(X_test)

# Evaluate
ev = EvalSurv(surv, t_test, e_test, censor_surv='km')
print("Test C-index:", ev.concordance_td())
print("Integrated Brier Score:", ev.integrated_brier_score(np.linspace(t_test.min(), t_test.max(), 100)))

# 1) Load and clean data
df = pd.read_csv("/content/drive/MyDrive/project/data/survival_dataset.csv")
df = df.drop(columns=['partial_hazard'], errors='ignore')

# 2) Prepare features and targets
feature_names = [c for c in df.columns if c not in ['time','event']]
X = df[feature_names].values.astype('float32')
t = df['time'].values.astype('float32') + 1e-3  # avoid zero durations
e = df['event'].values.astype('int32')

# 3) Impute and scale
X = SimpleImputer(strategy='mean').fit_transform(X)
X = StandardScaler().fit_transform(X).astype('float32')

# 4) Split
X_train, X_test, t_train, t_test, e_train, e_test = train_test_split(
    X, t, e, test_size=0.2, random_state=42, stratify=e
)

# 5) Define model
net = tt.practical.MLPVanilla(X_train.shape[1], [64, 32], 1, batch_norm=True, dropout=0.1)
model = CoxPH(net, torch.optim.Adam)
model.optimizer.set_lr(1e-3)

# 6) Train
model.fit(X_train, (t_train, e_train), batch_size=32, epochs=100, verbose=True)

# 7) Compute baseline hazards
model.compute_baseline_hazards()

# 8) Predict survival curves
surv = model.predict_surv_df(X_test[:20].astype('float32'))

# 9) Check values
print("Survival curves shape:", surv.shape)
print("Any NaNs?", surv.isna().any().any())
print("Max:", surv.max().max(), "Min:", surv.min().min())

# Save trained DeepSurv model
model.save_model_weights("/content/drive/MyDrive/project/deepsurv_model.pt")

# ============================================
# ✅ Plot Survival Curves
# ============================================
import matplotlib.pyplot as plt

if not surv.isna().any().any():
    plt.figure(figsize=(10, 6))
    for i, col in enumerate(surv.columns[:5]):
        plt.plot(surv.index, surv[col], label=f"Patient {i}")
    plt.title("DeepSurv - Predicted Survival Curves")
    plt.xlabel("Time")
    plt.ylabel("Survival Probability")
    plt.legend()
    plt.grid(True)
    plt.show()
else:
    print("⚠️ Still NaNs in survival curves. Check training loss and input scaling.")

# ============================================
# Cell 6 — SHAP Analysis of DeepSurv Model
# ============================================

# 1) Prepare clean feature matrix (drop time/event)
feature_names = [c for c in df.columns if c not in ['time','event']]
X_raw = df[feature_names].values.astype('float32')

# 2) Impute missing values
imputer = SimpleImputer(strategy='mean')
X_imp = imputer.fit_transform(X_raw)

# 3) Scale features (same as training)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp).astype('float32')

# 4) Define SHAP-compatible prediction function
def risk_fn(x):
    x = np.asarray(x, dtype='float32')
    out = model.predict(x)  # log hazard ratio
    out = np.array(out).reshape(-1)
    if not np.isfinite(out).all():
        finite = out[np.isfinite(out)]
        med = np.median(finite) if finite.size > 0 else 0.0
        out = np.where(np.isfinite(out), out, med)
    return out

# 5) Select background and sample sets
background = X_scaled[:50]   # background for SHAP
sample = X_scaled[:10]       # samples to explain

# 6) Build KernelExplainer
explainer = shap.KernelExplainer(risk_fn, background)

# 7) Compute SHAP values
shap_values = explainer.shap_values(sample, nsamples=100)

# 8) Convert to array and fix shape
shap_arr = np.asarray(shap_values, dtype='float32')
if shap_arr.ndim == 1:
    shap_arr = shap_arr.reshape(1, -1)

# 9) Plot global SHAP summary
shap.summary_plot(shap_arr, sample, feature_names=feature_names)

# Save trained model
model.save_model_weights("/content/drive/MyDrive/project/deepsurv_model.pt")

# Features only (drop time/event)
feature_names = [c for c in df.columns if c not in ['time','event']]
X_raw = df[feature_names].values.astype('float32')

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_imp = imputer.fit_transform(X_raw)

# Scale features (same as training)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imp).astype('float32')

# Risk function: log hazard ratio from trained DeepSurv model
def risk_fn(x):
    x = np.asarray(x, dtype='float32')
    out = model.predict(x)  # DeepSurv linear predictor
    out = np.array(out).reshape(-1)
    if not np.isfinite(out).all():
        finite = out[np.isfinite(out)]
        med = np.median(finite) if finite.size > 0 else 0.0
        out = np.where(np.isfinite(out), out, med)
    return out

# Define the same network architecture used in training
in_features = X_scaled.shape[1]
net = tt.practical.MLPVanilla(in_features, [64, 32], 1, batch_norm=True, dropout=0.1)

# Recreate CoxPH model
model = CoxPH(net, torch.optim.Adam)
model.optimizer.set_lr(1e-3)

# Load trained weights
model.load_model_weights("/content/drive/MyDrive/project/deepsurv_model.pt")

# Background and sample sets
background = X_scaled[:50]
sample = X_scaled[:10]

# KernelExplainer
explainer = shap.KernelExplainer(risk_fn, background)

# Compute SHAP values
shap_values = explainer.shap_values(sample, nsamples=100)

# Convert to array
shap_arr = np.asarray(shap_values, dtype='float32')
if shap_arr.ndim == 1:
    shap_arr = shap_arr.reshape(1, -1)

# Global summary plot
shap.summary_plot(shap_arr, sample, feature_names=feature_names)

# ============================================
# Cell 5 — Local SHAP Force Plots
# ============================================

# 1) Compute risk scores for all patients
risk_scores = model.predict(X_scaled)

# 2) Sort patients by risk
sorted_idx = np.argsort(risk_scores)

# 3) Select 3 low-risk and 3 high-risk patients
low_risk_idx = sorted_idx[:3]
high_risk_idx = sorted_idx[-3:]

# 4) Generate force plots
print("Low-risk patients:", low_risk_idx)
print("High-risk patients:", high_risk_idx)

for idx in low_risk_idx:
    shap.force_plot(
        explainer.expected_value,
        shap_arr[idx],
        X_scaled[idx],
        feature_names=feature_names,
        matplotlib=True
    )

for idx in high_risk_idx:
    shap.force_plot(
        explainer.expected_value,
        shap_arr[idx],
        X_scaled[idx],
        feature_names=feature_names,
        matplotlib=True
    )

# ============================================
# Cell 6 — Compare SHAP Importance with CoxPH Coefficients (fixed)
# ============================================


# 1) Define a simple linear network for CoxPH
in_features = X_scaled.shape[1]
net_cox = nn.Sequential(nn.Linear(in_features, 1))  # plain linear layer

# 2) Fit CoxPH model
cox_model = CoxPH(net_cox, torch.optim.Adam)
cox_model.optimizer.set_lr(1e-3)

cox_model.fit(
    X_scaled,
    (df['time'].values.astype('float32'), df['event'].values.astype('int32')),
    batch_size=32,
    epochs=50,
    verbose=False
)

# 3) Extract CoxPH coefficients
weights = net_cox[0].weight.detach().numpy().flatten()

# 4) Compute mean absolute SHAP values (global importance)
shap_importance = np.abs(shap_arr).mean(axis=0)

# 5) Compare rankings
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'CoxPH_Coefficient': weights,
    'MeanAbs_SHAP': shap_importance
}).sort_values('MeanAbs_SHAP', ascending=False)

print(importance_df.head(10))  # top 10 features

# 6) Plot comparison
plt.figure(figsize=(10,6))
plt.scatter(importance_df['CoxPH_Coefficient'], importance_df['MeanAbs_SHAP'])
plt.xlabel("CoxPH Coefficient")
plt.ylabel("Mean Absolute SHAP Value")
plt.title("Comparison of CoxPH vs SHAP Feature Importance")
plt.grid(True)
plt.show()